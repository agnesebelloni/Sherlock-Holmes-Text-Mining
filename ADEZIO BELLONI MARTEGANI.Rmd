---
title: 'PROGETTO DATA MINING: Text mining Sherlock Holmes'
author: "Giuditta ADEZIO, Agnese BELLONI, Luca MARTEGANI"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Questo progetto mira a un'analisi approfondita dei libri di Sherlock Holmes attraverso tecniche di text mining.

## ANALISI ESPLORATIVA CON MONDO TIDYVERSE

Dopo aver importato alcune library, che ci saranno utili nelle analisi successive, e aver caricato i documenti attraverso la library sherlock presente nella directory di Github riportata nello script, selezioniamo un solo libro per analizzarne i singoli termini. Il libro in questione è "A Study In Scarlet".

```{r}
# devtools::install_github("EmilHvitfeldt/sherlock")
library(sherlock)
library(tidyverse)
library(stringr)
library(tidytext)

sherlock::holmes
str(holmes)
summary(holmes)
head(holmes)

unique(holmes$book)

astudyinscarlet <- holmes %>% 
                      filter(book == 'A Study In Scarlet')
astudyinscarlet <- astudyinscarlet %>%
                             select(text) %>%
                               mutate(chapter = cumsum(str_detect(text, "^CHAPTER")))
astudyinscarlet %>%
  unnest_tokens(word, text)

```

Selezionando una lista di libri di nostro interesse abbiamo quindi ripetuto la stessa cosa che avevamo già fatto per il primo (A Study in Scarlet) mettendo tutti i termini in un tibble con le singole parole, affiancate a ogni libro in cui esse vengono utilizzate.

```{r}
titles <- c('A Study In Scarlet',"A Scandal in Bohemia", 
            "The Adventure of the Devil's Foot", "The Hound of the Baskervilles", 
            "The Red-Headed League", "His Last Bow")

books <- list(filter(holmes, book == "A Study In Scarlet"), 
              filter(holmes, book == "A Scandal in Bohemia"),
              filter(holmes, book == "The Adventure of the Devil's Foot"),
              filter(holmes, book == "The Hound of the Baskervilles"),
              filter(holmes, book == "The Red-Headed League"),
              filter(holmes, book == "His Last Bow") )

series <- tibble()

for (i in seq_along(titles)) {
  clean <- books[[i]]%>%
    select(text)%>%
    mutate(chapter = cumsum(str_detect(text, "^CHAPTER"))) %>%
    unnest_tokens(word, text) %>%
    mutate(book = titles[i]) %>%
    select(book, everything())
  
  series <- rbind(series, clean)
  
}

series
series$book <- factor(series$book, levels = rev(titles))
```

Adesso eseguendo l'azione di eliminazione delle stopwords contiamo quante volte ogni termine viene ripetuto, considerando tutti i libri scelti in precendenza congiuntamente.

```{r}
numero <- series %>% 
  anti_join(stop_words) %>% 
    count(word, sort = T)
numero
```

Ne ricaviamo che le parole più utilizzate sono Holmes e Sir che risultano ripetute molte più volte della terza parola in quest'ordine (time).

Rappresentiamo graficamente le parole più utilizzate divise per libro.

```{r}
library(RColorBrewer)

df <- series %>%
        anti_join(stop_words) %>%
        group_by(book) %>%
        count(word, sort = TRUE) %>%
        top_n(10) %>%
        ungroup() %>%
        mutate(book = factor(book, levels = titles),
              text_order = nrow(.):1)
  
ggplot(df, aes(reorder(word, n), n, fill = book)) +
  geom_bar(stat = "identity", color = 'black') +
  facet_wrap(~ book, scales = "free_y") +
  labs(x = NULL, y = "Frequency") +
  coord_flip()+
  scale_fill_brewer(palette = 'Pastel2') + 
  theme(legend.position="none") 


```

Essendo la scala utilizzata sulle x uguale per tutti i libri possiamo ben vedere come la frequenza delle parole del libro 'The Hound of the Baskervilles' è di molto maggiore rispetto alle parole degli altri libri.

Calcoliamo di seguito le frequenze relative di utilizzo dei termini rispetto a tutti i libri e relativamente a ogni singolo libro.

```{r}
sherlock_pct <- series %>%
  anti_join(stop_words) %>%
  count(word) %>%
  transmute(word, all_words = n / sum(n))

(frequency <- series %>%
  anti_join(stop_words) %>%
  count(book, word) %>%
  mutate(book_words = n / sum(n)) %>%
  left_join(sherlock_pct) %>%
  arrange(desc(book_words)) %>%
  ungroup())

```

Notiamo facilmente da questa tabella che il libro The Hound of the Baskervilles probabilmente è molto più lungo degli altri o ripete molto spesso le stesse parole dal momento che 9 su 10 delle parole in top 10 assoluta appartengono a tale libro, ciò coerentemente con quanto detto riguardo il grafico precedente.

Poi calcoliamo un tibble che riporta la correlazione tra le parole di ogni libro e quelle totali calcolandone anche il pvalue per valutarne la significatività.

```{r}

(corr <- frequency %>%
        group_by(book) %>%
           summarize(correlation = cor(book_words, all_words),
              p_value = cor.test(book_words, all_words)$p.value) %>%
           arrange(desc(correlation)))

```

La correlazione maggiore si ha con le parole di The Hound of the Baskervilles.

Dal tibble delle frequenze calcolato in precedenza, salviamo adesso le 60 parole più utilizzate in ogni libro e rappresentiamo graficamente le percentuali di parole ripetute di più in generale colorandole per libro.

```{r}
top_60 <- frequency %>%
  arrange(desc(book_words)) %>%
  top_n(60) 

ggplot(top_60, aes(x = reorder(word, -book_words), y = book_words, fill = book)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL, y = "PERCENTUALE DI PAROLE TOTALI") +
  scale_fill_manual(values = c('navy', 'seagreen2', 'blue', 'purple1', 'magenta', 'olivedrab2', 'violet')) +
  theme_minimal() 
```

Per concludere quest'analisi iniziale, rappresentiamo graficamente con una wordcloud le parole più usate in questi sei libri scelti.

```{r}
library(wordcloud)
set.seed(123)

wordcloud(numero$word, numero$n, min.freq = 40, colors = viridis::plasma(30))
```

Come precedentemente detto dal grafico si evince che le parole Holmes e sir sono state utilizzate in numero molto più elevato rispetto alle altre. La parola sir ci inizia già a suggerire in che contesto storico è ambientata la storia. 

## ANALISI DEL CORPUS CON DOCUMENT TERM

Per la successiva analisi importiamo le library necessarie al text mining senza tidyverse e carichiamo 56 libri, molti dei quali si trovavano anche nella library utilizzata fino a questo momento, come file di testo, per creare in maniera più comoda un Corpus. 
Abbiamo eseguito la seguente operazione perché dentro la library Sherlock i testi si trovavano tutti nello stesso documento pertanto la document-term non era realizzabile.

```{r}
library(tm)
library(SnowballC)

dati <- Corpus(DirSource("C:\\Users\\giudi\\OneDrive\\Desktop\\uni\\data mining\\sherlock"))

```

A questo punto attuiamo le tecniche base di data mining: togliamo la punteggiatura, i numeri e trasformiamo tutto in minuscolo prima di rimuovere le stopwords di base e gli spazi bianchi.
```{r}
dati <- tm_map(dati, removePunctuation)
dati <- tm_map(dati, removeNumbers)
dati <- tm_map(dati, content_transformer(tolower))

dati <- tm_map(dati, removeWords, stopwords('english'))
dati <- tm_map(dati, stripWhitespace)
```

Filtriamo le parole nel corpus, trasformato in testo, per cercare a quali parole appartengano alcune radici che nello stemming (riportato in seguito) sono state tagliate in maniera non del tutto corretta, in modo tale da trasformarle successivamente nelle parole giuste.
```{r}
testo_corpus <- sapply(dati, as.character)
'unlist(str_extract_all(testo_corpus, "\\bspeckl\\w*\\b"))
 unlist(str_extract_all(testo_corpus, "\\blodg\\w*\\b"))
 unlist(str_extract_all(testo_corpus, "\\bnobl\\w*\\b"))
 unlist(str_extract_all(testo_corpus, "\\bboscomb\\w*\\b"))
 unlist(str_extract_all(testo_corpus, "\\bbeech\\w*\\b")) 
 unlist(str_extract_all(testo_corpus, "\\bgrang\\w*\\b"))
 unlist(str_extract_all(testo_corpus, "\\bholmesesesd\\w*\\b")) 
 unlist(str_extract_all(testo_corpus, "\\bdthe\\w*\\b"))
 unlist(str_extract_all(testo_corpus, "\\bcri\\w*\\b"))
 unlist(str_extract_all(testo_corpus, "\\bdno\\w*\\b")) 
 unlist(str_extract_all(testo_corpus, "\\bleav\\w*\\b"))
 unlist(str_extract_all(testo_corpus, "\\bbusi\\w*\\b"))
 unlist(str_extract_all(testo_corpus, "\\bcours\\w*\\b"))
 unlist(str_extract_all(testo_corpus, "\\bpolic\\w*\\b"))
 unlist(str_extract_all(testo_corpus, "\\bdm\\w*\\b")) # ci sono dentro un sacco di  parole ma la più presente è dmy per questo successivamente la trasformeremo in dmy
 unlist(str_extract_all(testo_corpus, "\\banothing\\w*\\b"))
 unlist(str_extract_all(testo_corpus, "\\bmand\\w*\\b"))
 unlist(str_extract_all(testo_corpus, "\\binde\\w*\\b")) 
 unlist(str_extract_all(testo_corpus, "\\battent\\w*\\b"))'


# per evitare che li stampasse tutti li abbiamo messi sotto forma di commenti
```

Per migliorare le capacità di text mining passiamo alla fase di stemming per raggruppare tutte le parole con lo stesso significato e ridurre la dimensionalità della matrice che creeremo succesivamente.
Come si può ben vedere abbiamo trattato in maniera più accurata le parole più frequenti che erano state tagliate male dallo stemming di R.
```{r}
dati <- tm_map(dati, stemDocument)

dati <- tm_map(dati, content_transformer(gsub), pattern = 'baskervill', 
               replacement = 'baskerville')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'adventur', 
               replacement = 'adventure')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'studi', 
               replacement = 'study')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'hous', 
               replacement = 'house')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'misteri', 
               replacement = 'mistery')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'ladi', 
               replacement = 'lady')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'priori', 
               replacement = 'priority')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'holm', 
               replacement = 'holmes')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'speckl', 
               replacement = 'speckled')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'mysteri', 
               replacement = 'mystery')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'lodg', 
               replacement = 'lodge')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'nobl', 
               replacement = 'noble')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'boscomb', 
               replacement = 'boscombe')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'grang', 
               replacement = 'grange')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'leagu', 
               replacement = 'league')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'therefor', 
               replacement = 'therefore')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'outsid', 
               replacement = 'outside')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'forc', 
               replacement = 'force')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'terribl', 
               replacement = 'terrible')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'littl', 
               replacement = 'little')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'everi', 
               replacement = 'every')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'howev', 
               replacement = 'however')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'cri', 
               replacement = 'crime')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'noth', 
               replacement = 'nothing')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'holmesesesd', 
               replacement = 'holmes')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'possibl', 
               replacement = 'possible')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'leav', 
               replacement = 'leave')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'leavee', 
               replacement = 'leave')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'someth', 
               replacement = 'something')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'cour', 
               replacement = 'course')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'coursese', 
               replacement = 'course')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'courses', 
               replacement = 'course')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'busi', 
               replacement = 'business')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'examin', 
               replacement = 'examine')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'polic', 
               replacement = 'police')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'tri', 
               replacement = 'try')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'rememb', 
               replacement = 'remember')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'realli', 
               replacement = 'really')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'glanc', 
               replacement = 'glance')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'observ', 
               replacement = 'observe')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'strang', 
               replacement = 'strange')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'lestrad', 
               replacement = 'lestrade')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'alreadi', 
               replacement = 'already')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'thatd', 
               replacement = 'that')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'anyth', 
               replacement = 'anything')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'watsond', 
               replacement = 'watson')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'sinc', 
               replacement = 'since')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'evid', 
               replacement = 'evident')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'tabl', 
               replacement = 'table')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'minut', 
               replacement = 'minute')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'surpris', 
               replacement = 'surprised')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'togeth', 
               replacement = 'together')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'larg', 
               replacement = 'large')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'natur', 
               replacement = 'nature')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'naturee', 
               replacement = 'nature')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'largee', 
               replacement = 'large')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'surpriseded', 
               replacement = 'surprised')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'minutee', 
               replacement = 'minute')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'tablee', 
               replacement = 'table')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'holmesesd', 
               replacement = 'holmes')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'carri', 
               replacement = 'carry')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'crimeme', 
               replacement = 'crime')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'believ', 
               replacement = 'believe')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'tablee', 
               replacement = 'table')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'dmi', 
               replacement = 'dmy')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'perhap', 
               replacement = 'perhaps')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'imagin', 
               replacement = 'imagine')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'suppos', 
               replacement = 'suppose')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'sird', 
               replacement = 'sir')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'stori', 
               replacement = 'story')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'inde', 
               replacement = 'indeed')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'warranti', 
               replacement = 'warranty')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'voic', 
               replacement = 'voice')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'abl', 
               replacement = 'able')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'offic', 
               replacement = 'office')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'chanc', 
               replacement = 'chance')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'figur', 
               replacement = 'figure')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'posit', 
               replacement = 'position')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'caus', 
               replacement = 'cause')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'bodi', 
               replacement = 'body')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'absolut', 
               replacement = 'absolutely')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'attent', 
               replacement = 'attention')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'resum', 
               replacement = 'resume')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'genuin', 
               replacement = 'genuine')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'deplor', 
               replacement = 'deplorable')
```

Togliamo tutte quelle parole che riteniamo non essere utili ai fini dell'analisi, come verbi o sostantivi molto comuni, in modo tale che queste non influenzino i risultati della stessa.
Abbiamo volutamente lasciato nell'analisi alcune parole con le caratteristiche sopra riportate (che potevano essere eliminate), ma scritte in forma arcaica, perchè utili a spiegare il contesto temporale del libro. Riportiamo ad esempio le parole alway (always) e dyou (you).
```{r}
myStopwords <- c("will", "say", "think", "see", "said", "must", "may", "look", "little",
                 "know", "come", "can", "came", "ask", "one", "two", "get", "shall",
                 "might", "like", "find", "made", "make", "now", "heard", "thought",
                 "way", "just", "much", "tell", "even", "turn", "long", "saw", "never",
                 "first", "last", "dit", "left", "found", "take", "upon", "give",
                 "end", "took", "yet", "however", "without", "hard", "man", "knew", 
                 "leave", "something", "dand", "use", "every", "told", "thing",
                 "rather", "got", "seen", "pass", "call", "done", "let", "put",
                 "three", "still", "also", "went", "really", "though", "brought",
                 "already", "that", "itd", "since", "behind", "gave", "gone", "himd",
                 "besid", "anothing", "mand", "mani")
# mani(many) non è stato trasformato ma direttamente eliminato

dati <- tm_map(dati, removeWords, myStopwords)
```


Creiamo una Document-Term matrix, cioè una matrice strutturata che rappresenta i testi che stiamo analizzando avente i documenti come righe e i termini come colonne.
```{r}
document_term <- DocumentTermMatrix(dati)

str(document_term)
inspect(document_term[1:2, 100:105])
```

Calcoliamo la frequenza totale dei termini, ordinandoli successivamente per frequenza decrescente, a questo punto visualizziamo i termini più frequenti e quelli meno frequenti.
```{r}
freq <- colSums(as.matrix(document_term))
length(freq)

ord <- order(freq, decreasing = T)
freq[head(ord, 100)] 
freq[tail(ord)]
```
Ne risulta che i termini più frequenti sono holmes, hand, watson, time, face, room, house, case, crime con holmes con frequenza esponenzialmente più grande di tutte le altre.

Selezioniamo le prime 50 parole con frequenza maggiore nei testi per le successive analisi. Visualizziamo in forma scritta le parole più frequenti e le associazioni con le parole holmes e watson che ci sembrano rilevanti, essendo essi i protagonisti della storia.
```{r}
mfw <- freq[head(ord, 50)]

wf <- data.frame(term = names(freq),
                 occurrences = freq)

findFreqTerms(document_term, lowfreq = 300)
findAssocs(document_term, 'holmes', 0.5)
findAssocs(document_term, 'watson', 0.5)
```

Creiamo un istogramma con le parole più frequenti, prendendo quelle che si presentano almeno 500 volte.
```{r}
library(ggplot2)
ggplot(subset(wf, freq > 500), aes(term, occurrences)) +
  geom_bar(stat = 'identity', fill = 'slateblue') +
  coord_flip() +
  labs(title = 'parole più frequenti')
```

Come precedentemente detto, il termine con maggiore frequenza è holmes, che appare quasi 3 volte in più della seconda parola più frequente; mi aspetto pertanto che, nella successiva clusterizzazione, possa apparire come cluster a sé.


Di seguito un altro grafico per le parole più frequenti, questa volta con wordcloud da cui si evince ancor più chiaramente quanto già detto per la parola holmes.
```{r}
library(wordcloud)
wordcloud(names(freq), freq, min.freq = 160, colors = viridis::plasma(100))
```

Dal wordcloud vediamo che, a parte holmes e watson, che comunque sono i protagonisti della storia quindi chiaramente sono due delle parole più dette, molte delle parole rimandano a quello che sherlock holmes fa di mestiere cioè l'investigatore come ad esempio crime, case, inspector. Poi abbiamo alcune parole legate al tempo come time e instant, alcune parole legate ai luoghi e alle persone che sherlock spesso descrive e come abbiamo già detto abbiamo lasciato un po' di parole che ci sembravano potessero far capire a chi legge in che posto (london) e in che momento è ambientata la storia, come tutte le parole che nell'inglesse corrente hanno perso la d iniziale (ad esempio dbut).

Creiamo una Term-Document matrix invertendo la matrice Document-Term con solamente le 30 parole più frequenti trovate in precedenza, convertendola poi in forma matriciale per le successive analisi.
```{r}
new_document_term <- document_term[, names(mfw)]
matrix_tdm <- as.matrix(t(new_document_term))
```

Definiamo i cluster cominciando con il clustering gerarchico e ne rappresentiamo il dendogramma derivante.
```{r}
d_tdm <- dist(matrix_tdm)
gruppi_tdm <- hclust(d_tdm, method = 'ward.D2')
plot(gruppi_tdm, hang = -1, main = 'DENDROGRAMMA CLUSTER PAROLE')
```

Dividiamo in 4 gruppi e rappresentiamo la suddivisione di seguito. 
```{r}
cutree(gruppi_tdm, 4)

plot(gruppi_tdm, hang = -1, main = 'DENDROGRAMMA CLUSTER PAROLE')
par(lwd = 2)
rect.hclust(gruppi_tdm, k = 4, border = 'olivedrab2')
```

Il clustering gerarchico qui sopra riportato dividendo le parole in quattro cluster riporta due cluster molto meno numerosi degli altri. Se facciamo particolare attenzione uno dei due è il cluster contentente solo la parola holmes, come già supponevamo in precedenza, mentre l'altro riporta le parole watson e sir che comunque erano due delle parole con maggior frequenza. 

Facciamo la stessa suddivisione in gruppi anche con il metodo delle K-Medie, con K=4
```{r}
kfit_tdm <- kmeans(d_tdm, centers = 4, nstart = 2)

```

Rappresentiamo i 4 gruppi con il grafico clusplot.
```{r}
library(cluster)
library(viridis)
palette <- viridis_pal(option = "D")(length(unique(kfit_tdm$cluster)))
clusplot(as.matrix(d_tdm), kfit_tdm$cluster, color = T, shade = T, labels = 1, lines =            0, col.p = palette, main = 'PLOT CLUSTER PAROLE')
par(lwd = 1)
```
Possiamo ben visualizzare come la parola holmes sia ben separata dalle altre anche nella rappresentazione grafica, coerentemente con quanto già succedeva con il clustering gerarchico.

Visualizziamo i centroidi per ogni cluster e, successivamente, la dimensione degli stessi cluster.
```{r}
print(kfit_tdm)
kfit_tdm$size
```
La divisione con k-medie riporta 4 gruppi composti rispettivamente da 1, 24, 14, 11 parole.

Passiamo ora ad analizzare i documenti, creando una riprendendo la Document-Term matrix, cioè una matrice con i documenti come righe e i termini come colonne.
```{r}
inspect(new_document_term[1:3, 1:4])
matrix_dtm <- as.matrix(new_document_term)
```

Eseguiamo le stesse analisi fatte in precedenza per fare questa volta un clustering dei documenti e non delle parole. 
Abbiamo ritenuto opportuno fare 5 cluster, dal momento che i libri erano suddivisi in 5 macrostorie, di seguito la rappresentazione del clustering gerarchico.
```{r, echo=FALSE, out.width='80%'}
d_dtm <- dist(matrix_dtm)
gruppi_dtm <- hclust(d_dtm, method = 'ward.D2')

par(mfrow=c(1,1), mar=c(0.1,0.1,0.1,0.1)) 
plot(gruppi_dtm, hang = -1, main = '')
cutree(gruppi_dtm, 5)

par(lwd = 2)
rect.hclust(gruppi_dtm, k = 5, border = 'olivedrab2')


```

il clustering gerarchico ci restituisce 4 cluster abbastanza numerosi e uno abbastanza meno numeroso degli altri. 

```{r}
kfit_dtm <- kmeans(d_dtm, centers = 5, nstart = 2)

par(mfrow=c(1,1), mar = c(5.1, 4.1, 4.1, 2.1)) 

library(viridis)
palette <- viridis_pal(option = "D")(length(unique(kfit_dtm$cluster)))
clusplot(as.matrix(d_dtm), kfit_dtm$cluster, color = T, shade = T, labels = 1, 
         lines = 0, col.p = palette, main = 'CLUSTER DOCUMENTI')

par(lwd = 1)

```

I cluster così realizzati sembrano abbastanza equi, difatti le unità sono più o meno equidistribuite cosa che, come sottolineato, non avveniva allo stesso modo con il clustering gerarchico in precedenza. 


```{r}
print(kfit_dtm)
kfit_dtm$size
```

