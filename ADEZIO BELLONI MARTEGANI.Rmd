---
title: 'PROGETTO DATA MINING: Text mining Sherlock Holmes'
author: "Giuditta ADEZIO, Agnese BELLONI, Luca MARTEGANI"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Questo progetto mira a un'analisi approfondita dei libri di Sherlock Holmes attraverso tecniche di text mining.

## ANALISI ESPLORATIVA CON MONDO TIDYVERSE

Dopo aver importato alcune library, che ci saranno utili nelle analisi successive, e aver importato i documenti attraverso la library sherlock presente nella directory di Github suddetta, selezioniamo un solo libro per analizzarne i singoli termini. Il libro in questione è "A Study In Scarlet".

```{r}
# devtools::install_github("EmilHvitfeldt/sherlock")
library(sherlock)
library(tidyverse)
library(stringr)
library(tidytext)

sherlock::holmes
str(holmes)
summary(holmes)
head(holmes)

unique(holmes$book)

astudyinscarlet <- holmes %>% 
                      filter(book == 'A Study In Scarlet')
astudyinscarlet <- astudyinscarlet %>%
                             select(text) %>%
                               mutate(chapter = cumsum(str_detect(text, "^CHAPTER")))
astudyinscarlet %>%
  unnest_tokens(word, text)

```

Selezionando una lista di libri di nostro interesse abbiamo quindi ripetuto la stessa cosa che avevamo già fatto per il primo (A Study in Scarlet) mettendo tutti i termini in un tibble con le singole parole, affiancate a ogni libro in cui esse vengono utilizzate.

```{r}
titles <- c('A Study In Scarlet',"A Scandal in Bohemia", 
            "The Adventure of the Devil's Foot", "The Hound of the Baskervilles", 
            "The Red-Headed League", "His Last Bow")

books <- list(filter(holmes, book == "A Study In Scarlet"), 
              filter(holmes, book == "A Scandal in Bohemia"),
              filter(holmes, book == "The Adventure of the Devil's Foot"),
              filter(holmes, book == "The Hound of the Baskervilles"),
              filter(holmes, book == "The Red-Headed League"),
              filter(holmes, book == "His Last Bow") )

series <- tibble()

for (i in seq_along(titles)) {
  clean <- books[[i]]%>%
    select(text)%>%
    mutate(chapter = cumsum(str_detect(text, "^CHAPTER"))) %>%
    unnest_tokens(word, text) %>%
    mutate(book = titles[i]) %>%
    select(book, everything())
  
  series <- rbind(series, clean)
  
}

series
series$book <- factor(series$book, levels = rev(titles))
```

Adesso eseguendo l'azione di eliminazione delle stopwords contiamo quante volte ogni termine viene ripetuto, considerando tutti i libri scelti in precendenza congiuntamente.

```{r}
numero <- series %>% 
  anti_join(stop_words) %>% 
    count(word, sort = T)
numero
```

Ne ricaviamo che le parole più utilizzate sono Holmes e Sir che risultano ripetute molte più volte della terza parola in quest'ordine (time).

Rappresentiamo graficamente le parole più utilizzate divise per libro.

```{r}
library(RColorBrewer)

df <- series %>%
        anti_join(stop_words) %>%
        group_by(book) %>%
        count(word, sort = TRUE) %>%
        top_n(10) %>%
        ungroup() %>%
        mutate(book = factor(book, levels = titles),
              text_order = nrow(.):1)
  
ggplot(df, aes(reorder(word, n), n, fill = book)) +
  geom_bar(stat = "identity", color = 'black') +
  facet_wrap(~ book, scales = "free_y") +
  labs(x = NULL, y = "Frequency") +
  coord_flip()+
  scale_fill_brewer(palette = 'Pastel2') + 
  theme(legend.position="none") 


```

Calcoliamo di seguito le frequenze relative di utilizzo dei termini rispetto a tutti i libri e relativamente a ogni singolo libro.

```{r}
sherlock_pct <- series %>%
  anti_join(stop_words) %>%
  count(word) %>%
  transmute(word, all_words = n / sum(n))

(frequency <- series %>%
  anti_join(stop_words) %>%
  count(book, word) %>%
  mutate(book_words = n / sum(n)) %>%
  left_join(sherlock_pct) %>%
  arrange(desc(book_words)) %>%
  ungroup())

```

Notiamo facilmente da questa tabella che il libro The Hound of the Baskervilles probabilmente è molto più lungo degli altri o ripete molto spesso le stesse parole dal momento che tutte le parole della top 10 di tale libro sono nelle prime 10 assolute.

Poi calcoliamo un tibble che riporta la correlazione tra le parole di ogni libro e quelle totali calcolandone anche il pvalue per valutarne la significatività.

```{r}

(corr <- frequency %>%
        group_by(book) %>%
           summarize(correlation = cor(book_words, all_words),
              p_value = cor.test(book_words, all_words)$p.value) %>%
           arrange(desc(correlation)))

```

La correlazione maggiore si ha con le parole di The Hound of the Baskervilles.

Dal tibble delle frequenze calcolato in precedenza salviamo adesso le 60 parole più utilizzate in ogni libro e rappresentiamo graficamente le percentuali di parole ripetute di più in generale colorandole per libro.

```{r}
top_60 <- frequency %>%
  arrange(desc(book_words)) %>%
  top_n(60) 

ggplot(top_60, aes(x = reorder(word, -book_words), y = book_words, fill = book)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL, y = "PERCENTUALE DI PAROLE TOTALI") +
  scale_fill_manual(values = c('navy', 'seagreen2', 'blue', 'purple1', 'magenta', 'olivedrab2', 'violet')) +
  theme_minimal() 
```

Per concludere quest'analisi iniziale, rappresentiamo graficamente con una wordcloud le parole più usate in questi sei libri scelti.

```{r}
library(wordcloud)
set.seed(123)

wordcloud(numero$word, numero$n, min.freq = 40, colors = viridis::plasma(30))
```

Come precedentemente detto dal grafico si evince che le parole Holmes e sir sono state utilizzate in numero molto più elevato rispetto alle altre.

## ANALISI DEL CORPUS CON DOCUMENT TERM

Per la successiva analisi importiamo le library necessarie al text mining e carichiamo gli stessi 56 libri, che si trovano nella library utilizzata fino a questo momento, come file di testo per creare in maniera più comoda un Corpus. Abbiamo eseguito la seguente operazione perché dentro la library Sherlock i testi si trovavano tutti nello stesso documento pertanto la document-term non era realizzabile.

```{r}
library(tm)
library(SnowballC)

dati <- Corpus(DirSource("C:/Users/Utente/Desktop/Uni/Data Mining/Sherlock Holmes"))

dati <- tm_map(dati, removePunctuation)
dati <- tm_map(dati, removeNumbers)
dati <- tm_map(dati, content_transformer(tolower))

dati <- tm_map(dati, removeWords, stopwords('english'))
dati <- tm_map(dati, stripWhitespace)
```

```{r}
testo_corpus <- sapply(dati, as.character)
```

Filtro le parole nel corpus trasformato in testo per cercare di cosa siano alcune radici, in modo tale da trasformarle successivamente nelle parole corrette

```{r}
(parole_speckl <- unlist(str_extract_all(testo_corpus, "\\bspeckl\\w*\\b")))
(parole_lodg <- unlist(str_extract_all(testo_corpus, "\\blodg\\w*\\b")))
(parole_nobl <- unlist(str_extract_all(testo_corpus, "\\bnobl\\w*\\b")))
(parole_boscomb <- unlist(str_extract_all(testo_corpus, "\\bboscomb\\w*\\b")))
(parole_beech <- unlist(str_extract_all(testo_corpus, "\\bbeech\\w*\\b"))) # faggio
(parole_grang <- unlist(str_extract_all(testo_corpus, "\\bgrang\\w*\\b")))
(parole_holm <- unlist(str_extract_all(testo_corpus, "\\bholmesesesd\\w*\\b"))) # mi dice numeric 0 ma se stampo freq ord mi dice che ce ne sono 340
(parole_dwhat <- unlist(str_extract_all(testo_corpus, "\\bdthe\\w*\\b"))) # non so cosa significa
(parole_dcrime <- unlist(str_extract_all(testo_corpus, "\\bcri\\w*\\b"))) 
(parole_not <- unlist(str_extract_all(testo_corpus, "\\bdno\\w*\\b"))) 
(parole_leavee <- unlist(str_extract_all(testo_corpus, "\\bleav\\w*\\b")))
(parole_busi <- unlist(str_extract_all(testo_corpus, "\\bbusi\\w*\\b")))
(parole_coursese <- unlist(str_extract_all(testo_corpus, "\\bcours\\w*\\b")))
(parole_polic <- unlist(str_extract_all(testo_corpus, "\\bpolic\\w*\\b")))
(parole_dmi <- unlist(str_extract_all(testo_corpus, "\\bdm\\w*\\b"))) # ci sono dentro un sacco di parole ma la più presente è dmy
(parole_dmi <- unlist(str_extract_all(testo_corpus, "\\banothing\\w*\\b"))) # non mi dice nulla non so come ci sia arrivato
unlist(str_extract_all(testo_corpus, "\\bmand\\w*\\b")) # penso sia man'd lo tolgo nelle stopwords
unlist(str_extract_all(testo_corpus, "\\binde\\w*\\b")) # indeed
unlist(str_extract_all(testo_corpus, "\\battent\\w*\\b"))
```

```{r}
dati <- tm_map(dati, stemDocument)

dati <- tm_map(dati, content_transformer(gsub), pattern = 'baskervill', 
               replacement = 'baskerville')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'adventur', 
               replacement = 'adventure')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'studi', 
               replacement = 'study')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'hous', 
               replacement = 'house')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'misteri', 
               replacement = 'mistery')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'ladi', 
               replacement = 'lady')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'priori', 
               replacement = 'priority')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'holm', 
               replacement = 'holmes')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'speckl', 
               replacement = 'speckled')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'mysteri', 
               replacement = 'mystery')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'lodg', 
               replacement = 'lodge')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'nobl', 
               replacement = 'noble')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'boscomb', 
               replacement = 'boscombe')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'grang', 
               replacement = 'grange')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'leagu', 
               replacement = 'league')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'therefor', 
               replacement = 'therefore')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'outsid', 
               replacement = 'outside')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'forc', 
               replacement = 'force')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'terribl', 
               replacement = 'terrible')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'littl', 
               replacement = 'little')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'everi', 
               replacement = 'every')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'howev', 
               replacement = 'however')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'cri', 
               replacement = 'crime')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'noth', 
               replacement = 'nothing')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'holmesesesd', 
               replacement = 'holmes')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'possibl', 
               replacement = 'possible')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'leav', 
               replacement = 'leave')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'leavee', 
               replacement = 'leave')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'someth', 
               replacement = 'something')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'cour', 
               replacement = 'course')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'coursese', 
               replacement = 'course')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'courses', 
               replacement = 'course')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'busi', 
               replacement = 'business')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'examin', 
               replacement = 'examine')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'polic', 
               replacement = 'police')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'tri', 
               replacement = 'try')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'rememb', 
               replacement = 'remember')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'realli', 
               replacement = 'really')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'glanc', 
               replacement = 'glance')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'observ', 
               replacement = 'observe')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'strang', 
               replacement = 'strange')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'lestrad', 
               replacement = 'lestrade')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'alreadi', 
               replacement = 'already')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'thatd', 
               replacement = 'that')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'anyth', 
               replacement = 'anything')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'watsond', 
               replacement = 'watson')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'sinc', 
               replacement = 'since')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'evid', 
               replacement = 'evident')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'tabl', 
               replacement = 'table')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'minut', 
               replacement = 'minute')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'surpris', 
               replacement = 'surprised')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'togeth', 
               replacement = 'together')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'larg', 
               replacement = 'large')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'natur', 
               replacement = 'nature')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'naturee', 
               replacement = 'nature')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'largee', 
               replacement = 'large')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'surpriseded', 
               replacement = 'surprised')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'minutee', 
               replacement = 'minute')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'tablee', 
               replacement = 'table')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'holmesesd', 
               replacement = 'holmes')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'carri', 
               replacement = 'carry')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'crimeme', 
               replacement = 'crime')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'believ', 
               replacement = 'believe')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'tablee', 
               replacement = 'table')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'dmi', 
               replacement = 'dmy')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'perhap', 
               replacement = 'perhaps')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'imagin', 
               replacement = 'imagine')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'suppos', 
               replacement = 'suppose')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'sird', 
               replacement = 'sir')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'stori', 
               replacement = 'story')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'inde', 
               replacement = 'indeed')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'warranti', 
               replacement = 'warranty')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'voic', 
               replacement = 'voice')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'abl', 
               replacement = 'able')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'offic', 
               replacement = 'office')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'chanc', 
               replacement = 'chance')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'figur', 
               replacement = 'figure')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'posit', 
               replacement = 'position')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'caus', 
               replacement = 'cause')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'bodi', 
               replacement = 'body')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'absolut', 
               replacement = 'absolutely')
dati <- tm_map(dati, content_transformer(gsub), pattern = 'attent', 
               replacement = 'attention')
```

Tolgo tutte quelle parole che ritengo non essere utili ai fini dell'analisi, come verbi o sostantivi molto comuni, in modo tale da non influenzare i risultati della stessa
Ho volutamente lasciato nell'analisi quelle parole con le caratteristiche sopra riportate, ma scritte in forma arcaica, perchè utili a spiegare il contesto temporale del libro

```{r}
myStopwords <- c("will", "say", "think", "see", "said", "must", "may", "look", "little",
                 "know", "come", "can", "came", "ask", "one", "two", "get", "shall",
                 "might", "like", "find", "made", "make", "now", "heard", "thought",
                 "way", "just", "much", "tell", "even", "turn", "long", "saw", "never",
                 "first", "last", "dit", "left", "found", "take", "upon", "give",
                 "end", "took", "yet", "however", "without", "hard", "man", "knew", 
                 "leave", "something", "dand", "use", "every", "told", "thing",
                 "rather", "got", "seen", "pass", "call", "done", "let", "put",
                 "three", "still", "also", "went", "really", "though", "brought",
                 "already", "that", "itd", "since", "behind", "gave", "gone", "himd",
                 "besid", "anothing", "mand", "mani")
# mani non l'ho trasformato, l'ho direttamente tolto poiché sarebbe many

dati <- tm_map(dati, removeWords, myStopwords)
```


Creo una Document-Term matrix, cioè una matrice con i documenti come righe e i termini come colonne

```{r}
document_term <- DocumentTermMatrix(dati)

str(document_term)
inspect(document_term[1:2, 100:105])
```

Calcolo la frequenza totale dei termini, ordinandoli successivamente per frequenza decrescente 

```{r}
freq <- colSums(as.matrix(document_term))
length(freq)

ord <- order(freq, decreasing = T)
freq[head(ord, 250)] # tra queste parole c'è alway che è una forma arcaica di always
freq[tail(ord)]
```

Seleziono le prime 50 parole con frequenza maggiore nei testi

```{r}
mfw <- freq[head(ord, 50)]

wf <- data.frame(term = names(freq),
                 occurrences = freq)

findFreqTerms(dtm, lowfreq = 100)
```

Creo un istogramma con le parole più frequenti, prendendo quelle che si presentano almeno 800 volte

```{r}
library(ggplot2)
ggplot(subset(wf, freq > 800), aes(term, occurrences)) +
  geom_bar(stat = 'identity', fill = 'slateblue') +
  coord_flip() +
  labs(title = 'parole più frequenti')
```

Il termine con maggiore frequenza è holmes, che appare quasi 3 volte in più della seconda parola più frequente; mi aspetto che, nella successiva clusterizzazione, possa apparire come cluster a sé


Di seguito un altro grafico per le parole più frequenti, questa volta con wordcloud

```{r}
library(wordcloud)
wordcloud(names(freq), freq, min.freq = 300, colors = viridis::plasma(100))
```

Creo una Term-Document matrix con solamente le 30 parole più frequenti trovate in precedenza, convertendola poi in matrice per le successive analisi

```{r}
new_document_term <- document_term[, names(mfw)]
matrix_tdm <- as.matrix(t(new_document_term))
```

Definisco i cluster cominciando con il clustering gerarchico e rappresento il dendogramma

```{r}
d_tdm <- dist(matrix_tdm)
gruppi_tdm <- hclust(d_tdm, method = 'ward.D2')
plot(gruppi_tdm, hang = -1)
```

Divido in 5 gruppi

```{r}
cutree(gruppi_tdm, 5)
```

Provo ad individuare i gruppi anche con il metodo delle K-Medie, con K=5

```{r}
kfit_tdm <- kmeans(d_tdm, centers = 5, nstart = 2)
par(lwd = 2)
rect.hclust(gruppi_tdm, k = 5, border = 'olivedrab2')
```

Rappresento i 4 gruppi con il grafico clusplot

```{r}
library(cluster)
palette <- viridis_pal(option = "D")(length(unique(kfit_tdm$cluster)))
clusplot(as.matrix(d_tdm), kfit_tdm$cluster, color = T, shade = T, labels = 1, lines = 0,
         col.p = palette, main = 'PLOT CLUSTER PAROLE')
par(lwd = 1)
```

Visualizzo i centroidi per ogni cluster e, successivamente, la dimensione degli stessi cluster

```{r}
print(kfit_tdm)
kfit_tdm$size
```


Passiamo ora ad analizzare i documenti, creando una Term-Document matrix, cioè una matrice con i termini come righe e i documenti come colonne
Questa è l'inversa della Document-Term matrix creata in precedenza

```{r}
matrix_dtm <- as.matrix(new_document_term)
```

Eseguo le stesse analisi fatte in precedenza

```{r}
d_tdm <- dist(matrix_tdm)
gruppi_tdm <- hclust(d_tdm, method = 'ward.D2')
```
```{r}
plot(gruppi_tdm, hang = -1)
cutree(gruppi_tdm, 4)
```
```{r}
kfit_tdm <- kmeans(d_tdm, centers = 4, nstart = 2)

par(lwd = 2)
rect.hclust(gruppi_tdm, k = 4, border = 'olivedrab2')
```
```{r}
library(viridis)
palette <- viridis_pal(option = "D")(length(unique(kfit_tdm$cluster)))
clusplot(as.matrix(d_tdm), kfit_tdm$cluster, color = T, shade = T, labels = 1, 
         lines = 0, col.p = palette, main = 'CLUSTER DOCUMENTI')
```
```{r}
par(lwd = 1)
print(kfit_tdm)
kfit_tdm$size
```

